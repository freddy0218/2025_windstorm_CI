{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "# Add the path to the directory containing the module\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from util.ml import baseline, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_rf_baseline = baseline.load_pickle('../../datas/proc/part1/rf/rf_rf_20_cdf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>10m_u_component_of_wind_std_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>surface_pressure_mean_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04</td>\n",
       "      <td>10m_v_component_of_wind_max_PCA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.12</td>\n",
       "      <td>mean_sea_level_pressure_max_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.14</td>\n",
       "      <td>surface_pressure_max_PCA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.08</td>\n",
       "      <td>surface_pressure_max_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08</td>\n",
       "      <td>geopotential_1000_std_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.12</td>\n",
       "      <td>2m_dewpoint_temperature_std_PCA_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>2m_dewpoint_temperature_std_PCA_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value                               name\n",
       "0   0.20  10m_u_component_of_wind_std_PCA_2\n",
       "1   0.14        surface_pressure_mean_PCA_2\n",
       "2   0.04  10m_v_component_of_wind_max_PCA_1\n",
       "3   0.12  mean_sea_level_pressure_max_PCA_2\n",
       "4   0.14         surface_pressure_max_PCA_1\n",
       "5   0.08         surface_pressure_max_PCA_2\n",
       "6   0.08        geopotential_1000_std_PCA_2\n",
       "7   0.12  2m_dewpoint_temperature_std_PCA_2\n",
       "8   0.08  2m_dewpoint_temperature_std_PCA_3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame({'value':best_rf_rf_baseline[2].feature_importances_, \n",
    "              'name':['10m_u_component_of_wind_std_PCA_2', 'surface_pressure_mean_PCA_2','10m_v_component_of_wind_max_PCA_1',\n",
    " 'mean_sea_level_pressure_max_PCA_2', 'surface_pressure_max_PCA_1','surface_pressure_max_PCA_2', 'geopotential_1000_std_PCA_2','2m_dewpoint_temperature_std_PCA_2','2m_dewpoint_temperature_std_PCA_3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itam/miniconda3/envs/myenv/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.4.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/itam/miniconda3/envs/myenv/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.4.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Find the folder name organized by seed number\n",
    "seed_docs = sorted(glob.glob('../../datas/seed_*/'))\n",
    "\n",
    "model_dict = []\n",
    "# Read files in a for loop\n",
    "for seed_doc in seed_docs:\n",
    "    model_RF_cdf_20 = pickle.load(open(seed_doc + 'model_random_forest/model_cdf_20.pkl', 'rb'))\n",
    "    model_RF_cdf_30 = pickle.load(open(seed_doc + 'model_random_forest/model_cdf_30.pkl', 'rb'))\n",
    "    model_RF_cdf_40 = pickle.load(open(seed_doc + 'model_random_forest/model_cdf_40.pkl', 'rb'))\n",
    "\n",
    "    model_RF_max_20 = pickle.load(open(seed_doc + 'model_random_forest/model_max_20.pkl', 'rb'))\n",
    "    model_RF_max_30 = pickle.load(open(seed_doc + 'model_random_forest/model_max_30.pkl', 'rb'))\n",
    "    model_RF_max_40 = pickle.load(open(seed_doc + 'model_random_forest/model_max_40.pkl', 'rb'))\n",
    "\n",
    "    model_dict.append({\n",
    "        'model_RF_cdf_20': model_RF_cdf_20,\n",
    "        'model_RF_cdf_30': model_RF_cdf_30,\n",
    "        'model_RF_cdf_40': model_RF_cdf_40,\n",
    "        'model_RF_max_20': model_RF_max_20,\n",
    "        'model_RF_max_30': model_RF_max_30,\n",
    "        'model_RF_max_40': model_RF_max_40\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the folder name organized by seed number\n",
    "seed_docs = sorted(glob.glob('../../datas/seed_*/'))\n",
    "\n",
    "data_dict = []\n",
    "# Read files in a for loop\n",
    "for seed_doc in seed_docs:\n",
    "    # Read input data\n",
    "    # Filter out the columns associated with convective_precipitation and large_scale_snowfall (model variables, not easily retrievable in observations)\n",
    "    Xtrain_40 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_train_40.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "    Xtrain_30 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_train_30.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "    Xtrain_20 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_train_20.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "    Xvalid_40 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_validation_40.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "    Xvalid_30 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_validation_30.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "    Xvalid_20 = baseline.filter_columns(pd.read_csv(seed_doc + 'X_validation_20.csv'),\"^(convective_precipitation_|large_scale_snowfall_)\")\n",
    "\n",
    "    # Read output data\n",
    "    ytrain_cdf = pd.read_csv(seed_doc + 'y_train_cdf.csv')\n",
    "    ytrain_max = pd.read_csv(seed_doc + 'y_train_max.csv')\n",
    "    yvalid_cdf = pd.read_csv(seed_doc + 'y_validation_cdf.csv')\n",
    "    yvalid_max = pd.read_csv(seed_doc + 'y_validation_max.csv')\n",
    "    \n",
    "    data = {\n",
    "        'Xtrain_40': Xtrain_40,\n",
    "        'Xtrain_30': Xtrain_30,\n",
    "        'Xtrain_20': Xtrain_20,\n",
    "        'Xvalid_40': Xvalid_40,\n",
    "        'Xvalid_30': Xvalid_30,\n",
    "        'Xvalid_20': Xvalid_20,\n",
    "        'ytrain_cdf': ytrain_cdf,\n",
    "        'ytrain_max': ytrain_max,\n",
    "        'yvalid_cdf': yvalid_cdf,\n",
    "        'yvalid_max': yvalid_max\n",
    "    }\n",
    "    data_dict.append(data)\n",
    "\n",
    "# Find the name of the columns that we removed from the data\n",
    "column_name_40 = pd.read_csv(seed_doc + 'X_train_40.csv').filter(regex=\"^(convective_precipitation_|large_scale_snowfall_)\").columns\n",
    "column_name_30 = pd.read_csv(seed_doc + 'X_train_30.csv').filter(regex=\"^(convective_precipitation_|large_scale_snowfall_)\").columns\n",
    "column_name_20 = pd.read_csv(seed_doc + 'X_train_20.csv').filter(regex=\"^(convective_precipitation_|large_scale_snowfall_)\").columns\n",
    "\n",
    "# All column indices\n",
    "varname_40 = pd.read_csv(seed_doc + 'X_train_40.csv').columns\n",
    "varname_30 = pd.read_csv(seed_doc + 'X_train_30.csv').columns\n",
    "varname_20 = pd.read_csv(seed_doc + 'X_train_20.csv').columns\n",
    "\n",
    "# Column indices for the removed variables\n",
    "filtindex_40 = [list(varname_40).index(list(column_name_40)[i]) for i in range(len(list(column_name_40)))]\n",
    "filtindex_30 = [list(varname_30).index(list(column_name_30)[i]) for i in range(len(list(column_name_30)))]\n",
    "filtindex_20 = [list(varname_20).index(list(column_name_20)[i]) for i in range(len(list(column_name_20)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the feature importance of the RF model\n",
    "feature_importance_20_cdf = []\n",
    "feature_importance_30_cdf = []\n",
    "feature_importance_40_cdf = []\n",
    "feature_importance_20_max = []\n",
    "feature_importance_30_max = []\n",
    "feature_importance_40_max = []\n",
    "\n",
    "for ind in range(len(model_dict)):\n",
    "    feature_importance_20_cdf.append(model_dict[ind]['model_RF_cdf_20'].feature_importances_)\n",
    "    feature_importance_30_cdf.append(model_dict[ind]['model_RF_cdf_30'].feature_importances_)\n",
    "    feature_importance_40_cdf.append(model_dict[ind]['model_RF_cdf_40'].feature_importances_)\n",
    "    feature_importance_20_max.append(model_dict[ind]['model_RF_max_20'].feature_importances_)\n",
    "    feature_importance_30_max.append(model_dict[ind]['model_RF_max_30'].feature_importances_)\n",
    "    feature_importance_40_max.append(model_dict[ind]['model_RF_max_40'].feature_importances_)\n",
    "\n",
    "# Find index where feature importance is greater than 0\n",
    "index_20_cdf = [[ind for ind, x in enumerate(feature_importance_20_cdf[i]) if x > 0] for i in range(len(feature_importance_20_cdf))]\n",
    "index_30_cdf = [[ind for ind, x in enumerate(feature_importance_30_cdf[i]) if x > 0] for i in range(len(feature_importance_30_cdf))]\n",
    "index_40_cdf = [[ind for ind, x in enumerate(feature_importance_40_cdf[i]) if x > 0] for i in range(len(feature_importance_40_cdf))]\n",
    "index_20_max = [[ind for ind, x in enumerate(feature_importance_20_max[i]) if x > 0] for i in range(len(feature_importance_20_max))]\n",
    "index_30_max = [[ind for ind, x in enumerate(feature_importance_30_max[i]) if x > 0] for i in range(len(feature_importance_30_max))]\n",
    "index_40_max = [[ind for ind, x in enumerate(feature_importance_40_max[i]) if x > 0] for i in range(len(feature_importance_40_max))]\n",
    "\n",
    "# Process the index to remove the columns associated with convective_precipitation and large_scale_snowfall\n",
    "index_20_filt_cdf = [baseline.filt_index_no_conv_snow(index_40_cdf[i],index_30_cdf[i],index_20_cdf[i],filtindex_40, filtindex_30, filtindex_20)[0] for i in range(len(index_20_cdf))]\n",
    "index_30_filt_cdf = [baseline.filt_index_no_conv_snow(index_40_cdf[i],index_30_cdf[i],index_20_cdf[i],filtindex_40, filtindex_30, filtindex_20)[1] for i in range(len(index_30_cdf))]\n",
    "index_40_filt_cdf = [baseline.filt_index_no_conv_snow(index_40_cdf[i],index_30_cdf[i],index_20_cdf[i],filtindex_40, filtindex_30, filtindex_20)[2] for i in range(len(index_40_cdf))]\n",
    "index_20_filt_max = [baseline.filt_index_no_conv_snow(index_40_max[i],index_30_max[i],index_20_max[i],filtindex_40, filtindex_30, filtindex_20)[0] for i in range(len(index_20_max))]\n",
    "index_30_filt_max = [baseline.filt_index_no_conv_snow(index_40_max[i],index_30_max[i],index_20_max[i],filtindex_40, filtindex_30, filtindex_20)[1] for i in range(len(index_30_max))]\n",
    "index_40_filt_max = [baseline.filt_index_no_conv_snow(index_40_max[i],index_30_max[i],index_20_max[i],filtindex_40, filtindex_30, filtindex_20)[2] for i in range(len(index_40_max))]\n",
    "            \n",
    "# Filter the input data with the feature importance\n",
    "data_dict_filt_cdf_rf = [baseline.filt_with_feature_importance(index_20_filt_cdf[ind], index_30_filt_cdf[ind], index_40_filt_cdf[ind], data_dict[ind]) for ind in range(len(data_dict))]\n",
    "data_dict_filt_max_rf = [baseline.filt_with_feature_importance(index_20_filt_max[ind], index_30_filt_max[ind], index_40_filt_max[ind], data_dict[ind]) for ind in range(len(data_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "bestmodel_20 = best_rf_rf_baseline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_dict_filt_cdf_rf[2][f'Xtrain_{int(20)}'])\n",
    "X_train_scaled = scaler.transform(data_dict_filt_cdf_rf[2][f'Xtrain_{int(20)}'])\n",
    "X_val_scaled = scaler.transform(data_dict_filt_cdf_rf[2][f'Xvalid_{int(20)}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_manual(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "    ss_residual = np.sum((y_true - y_pred)**2)\n",
    "    return 1 - (ss_residual / ss_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0011731793446621008,\n",
       " 0.020981857834270534,\n",
       " 0.024183007637168852,\n",
       " -0.25969290595737826,\n",
       " 0.02250231664585889,\n",
       " -0.025886143748740542,\n",
       " -0.12449226087826037,\n",
       " -0.013835018755522732,\n",
       " -0.1439959842950127,\n",
       " -0.17650393406430376,\n",
       " 0.06330607160659141,\n",
       " 0.15531777991584517,\n",
       " 0.14608810876850897,\n",
       " -0.02070772941018184,\n",
       " -0.1821248377915814]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r2_manual(data_dict_filt_cdf_rf[2]['yvalid_cdf'].iloc[:,i],bestmodel_20[2].predict(X_val_scaled)[:,i]) for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
