{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windstorm project\n",
    "This is a second out of several notebooks on Machine Learning-based prediction of severe surface winds associated with Extratropical Windstorms over different European geographical regions. Specifically, this study emphasizes how the temporal evolution characteristics of different storm internal and environmental predictors (\"history\") may contain useful information for quick evolution of severe wind potential overland.  This notebook preprocesses pre-landfall storm internal and environmental predictors data for predictive pattern discovery with nestedMLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import glob, os, gc\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Add the path to the directory containing the module\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from util.ml import baseline, metrics, preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and analyze time series data\n",
    "We start with 40 variables of storm internal and environmental (external) characteristics. We combine the predefined train-valid-test split and redo the split so that the validation data would sample all extreme storm cases at least once. The data consists with time series of storm internal and external characterstics taken from a moving grid box centered around the center of the windstorm. To reduce the dimensionality of the data, spatial moments (e.g., mean/max/std) of these fields are calculated and treated as separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the folder name organized by seed number\n",
    "seed_doc = sorted(glob.glob('../../datas/seed_revised_*/'))[0]\n",
    "\n",
    "# Load the data\n",
    "# Load the time series data\n",
    "df = pd.read_csv(seed_doc +'X_train_ts_all.csv')\n",
    "df_valid = pd.read_csv(seed_doc +'X_validation_ts_all.csv')\n",
    "df_test = pd.read_csv(seed_doc +'X_test_ts_all.csv')\n",
    "# Find the name for each column\n",
    "column_names = ([obj.split('_step_')[0] for obj in df.columns])\n",
    "# Unique names in the column name list\n",
    "unique_names = list(set(column_names))\n",
    "unique_names.remove('storm_index')\n",
    "# Create a dictionary for each unique name\n",
    "vardict = {\"name\": unique_names}\n",
    "vardict_valid = {\"name\": unique_names}\n",
    "vardict_test = {\"name\": unique_names}\n",
    "for name in unique_names:\n",
    "    vardict[name] = df[df.filter(regex=f\"^({name}_)\").columns].values\n",
    "    vardict_valid[name] = df_valid[df_valid.filter(regex=f\"^({name}_)\").columns].values\n",
    "    vardict_test[name] = df_test[df_test.filter(regex=f\"^({name}_)\").columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the data and split the data without delay. Since we have 63 storms, we will do 7 splits with 9 storms in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalidexp_dict = {}\n",
    "totalexp_dict = {}\n",
    "for name in unique_names:\n",
    "    # Concatenate the data\n",
    "    trainvalidexp_dict[name] = np.concatenate([vardict[name], vardict_valid[name]],axis=0)\n",
    "    totalexp_dict[name] = np.concatenate([trainvalidexp_dict[name], vardict_test[name]],axis=0)\n",
    "\n",
    "# Read and concatenate y data\n",
    "ytrain_cdf = pd.read_csv(seed_doc +'y_train_cdf.csv').values\n",
    "ytrain_max = pd.read_csv(seed_doc +'y_train_max.csv').values\n",
    "yvalid_cdf = pd.read_csv(seed_doc +'y_validation_cdf.csv').values\n",
    "yvalid_max = pd.read_csv(seed_doc +'y_validation_max.csv').values\n",
    "ytest_cdf = pd.read_csv(seed_doc +'y_test_cdf.csv').values\n",
    "ytest_max = pd.read_csv(seed_doc +'y_test_max.csv').values\n",
    "# Concatenate the y data\n",
    "y_cdf = np.concatenate([ytrain_cdf, yvalid_cdf],axis=0)\n",
    "y_max = np.concatenate([ytrain_max, yvalid_max],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_smoothing_and_pca_and_make_PCs_nosmooth(vardict, vardict_valid, vardict_test, unique_names, varsexp, train_sets, val_sets):\n",
    "    varsexp_filename = int(str(varsexp).split('.')[1])\n",
    "    if varsexp_filename < 10:\n",
    "        varsexp_filename = f'{varsexp_filename}0'\n",
    "    vardict_smooth = {\"name\": unique_names}\n",
    "    vardict_smooth_valid = {\"name\": unique_names}\n",
    "    vardict_smooth_test = {\"name\": unique_names}\n",
    "    for name in unique_names:\n",
    "        vardict_smooth[name] = np.asarray([obj for obj in vardict[name]])\n",
    "        vardict_smooth_valid[name] = np.asarray([obj for obj in vardict_valid[name]])\n",
    "        vardict_smooth_test[name] = np.asarray([obj for obj in vardict_test[name]])\n",
    "\n",
    "    trainvalidexp_dict = {}\n",
    "    for name in unique_names:\n",
    "        # Concatenate the data\n",
    "        trainvalidexp_dict[name] = np.concatenate([vardict_smooth[name], vardict_smooth_valid[name]],axis=0)\n",
    "\n",
    "    pca_dict = {}\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "    for iname in unique_names:\n",
    "        pca, mean, std = preproc.train_PCA(trainvalidexp_dict[iname])\n",
    "        pca_dict[iname] = pca\n",
    "        mean_dict[iname] = mean\n",
    "        std_dict[iname] = std\n",
    "    baseline.save_models(pca_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pca/pcaall.pkl')\n",
    "    baseline.save_models(mean_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/scaler/meanall.pkl')\n",
    "    baseline.save_models(std_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/scaler/stdall.pkl')\n",
    "\n",
    "    # Create a dictionary for each unique name\n",
    "    vardict_train = {\"name\": unique_names}\n",
    "    vardict_valid = {\"name\": unique_names}\n",
    "    \n",
    "    for i in range(7):\n",
    "        for name in unique_names:\n",
    "            vardict_train[name] = trainvalidexp_dict[name][train_sets[i]]\n",
    "            vardict_valid[name] = trainvalidexp_dict[name][val_sets[i]]\n",
    "        # Store the data\n",
    "        baseline.save_models(vardict_train,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/tsall_train_split_{i}.pkl')\n",
    "        baseline.save_models(vardict_valid,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/tsall_val_split_{i}.pkl')\n",
    "        \n",
    "    vardict_trains = []\n",
    "    vardict_valids = []    \n",
    "    for i in range(7):\n",
    "        vardict_trains.append(baseline.load_pickle(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/tsall_train_split_{i}.pkl'))\n",
    "        vardict_valids.append(baseline.load_pickle(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/tsall_val_split_{i}.pkl'))\n",
    "\n",
    "    PCloadings_train = []\n",
    "    PCloadings_valid = []\n",
    "    PCloadings_test = []\n",
    "\n",
    "    # Produce the PCA transformed data\n",
    "    for iseed in tqdm(range(7)):\n",
    "        PCdicts_train = {}\n",
    "        PCdicts_valid = {}\n",
    "        PCdicts_test = {}\n",
    "        for iname in unique_names:\n",
    "            #--------- FIHT: Remove this methods because it overcomplicates FFS ----------------\n",
    "            # # Find the number of components that explain 99% of the variance\n",
    "            atg = np.abs(pca_dict[iname].explained_variance_ratio_.cumsum()-varsexp).argmin()\n",
    "            #atg = 10\n",
    "            # Load the trained mean and standard deviation\n",
    "            trainmean = mean_dict[iname]\n",
    "            # Read the train/valid/test data\n",
    "            traindata = vardict_trains[iseed][iname]\n",
    "            validdata = vardict_valids[iseed][iname]\n",
    "            testdata = vardict_smooth_test[iname]\n",
    "            # PCA transform\n",
    "            temptrain = pca_dict[iname].transform(traindata)[:,:atg+1]\n",
    "            tempvalid = preproc.myPCA_projection_sen(pca_dict,iname,validdata,trainmean)[:,:atg+1]\n",
    "            temptest =  preproc.myPCA_projection_sen(pca_dict,iname,testdata,trainmean)[:,:atg+1]\n",
    "            # Standardize the data\n",
    "            PCdicts_train[iname] = (temptrain-np.mean(temptrain))/np.std(temptrain)\n",
    "            PCdicts_valid[iname] = (tempvalid-np.mean(temptrain))/np.std(temptrain)\n",
    "            PCdicts_test[iname] = (temptest-np.mean(temptrain))/np.std(temptrain)\n",
    "        PCloadings_train.append(PCdicts_train)\n",
    "        PCloadings_valid.append(PCdicts_valid)\n",
    "        PCloadings_test.append(PCdicts_test)\n",
    "\n",
    "    baseline.save_models(PCloadings_train,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_train.pkl')\n",
    "    baseline.save_models(PCloadings_valid,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_valid.pkl')\n",
    "    baseline.save_models(PCloadings_test,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained\n",
    "varsexp = 0.75\n",
    "# Store the indices for different storms\n",
    "storm_indices = np.arange(0,56)\n",
    "# Set the random seed\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(storm_indices)\n",
    "# Create 7 folds\n",
    "kf = KFold(n_splits=7, shuffle=False, random_state=None)  # Shuffle was already done\n",
    "# Store the indices for each fold\n",
    "train_sets, val_sets = [], []\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(storm_indices)):\n",
    "    train_set = storm_indices[train_idx]\n",
    "    val_set = storm_indices[val_idx]\n",
    "    train_sets.append(train_set)\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.30it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for varsexp in [0.75,0.80,0.85,0.90,0.95]:\n",
    "    do_smoothing_and_pca_and_make_PCs_nosmooth(vardict, vardict_valid, vardict_test, unique_names, varsexp, train_sets, val_sets)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
