{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import glob, os, gc\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Add the path to the directory containing the module\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from util.ml import baseline, metrics, preproc\n",
    "\n",
    "from scipy.fftpack import fft, ifft, fftshift, ifftshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the folder name organized by seed number\n",
    "seed_doc = sorted(glob.glob('../../datas/seed_revised_*/'))[0]\n",
    "\n",
    "# Load the data\n",
    "# Load the time series data\n",
    "df = pd.read_csv(seed_doc +'X_train_ts_all.csv')\n",
    "df_valid = pd.read_csv(seed_doc +'X_validation_ts_all.csv')\n",
    "df_test = pd.read_csv(seed_doc +'X_test_ts_all.csv')\n",
    "# Find the name for each column\n",
    "column_names = ([obj.split('_step_')[0] for obj in df.columns])\n",
    "# Unique names in the column name list\n",
    "unique_names = list(set(column_names))\n",
    "unique_names.remove('storm_index')\n",
    "# Create a dictionary for each unique name\n",
    "vardict = {\"name\": unique_names}\n",
    "vardict_valid = {\"name\": unique_names}\n",
    "vardict_test = {\"name\": unique_names}\n",
    "for name in unique_names:\n",
    "    vardict[name] = df[df.filter(regex=f\"^({name}_)\").columns].values\n",
    "    vardict_valid[name] = df_valid[df_valid.filter(regex=f\"^({name}_)\").columns].values\n",
    "    vardict_test[name] = df_test[df_test.filter(regex=f\"^({name}_)\").columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_smoothing(timeseries, cutoff):\n",
    "    rft = np.fft.rfft(timeseries)\n",
    "    rft[cutoff:] = 0   # Note, rft.shape = 19\n",
    "    y_smooth = np.fft.irfft(rft)\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_smoothing_and_pca_and_make_PCs(vardict, vardict_valid, vardict_test, unique_names, F_cutoff, varsexp, train_sets, val_sets):\n",
    "    varsexp_filename = int(str(varsexp).split('.')[1])\n",
    "    vardict_smooth = {\"name\": unique_names}\n",
    "    vardict_smooth_valid = {\"name\": unique_names}\n",
    "    vardict_smooth_test = {\"name\": unique_names}\n",
    "    for name in unique_names:\n",
    "        vardict_smooth[name] = np.asarray([fourier_smoothing(obj,F_cutoff) for obj in vardict[name]])\n",
    "        vardict_smooth_valid[name] = np.asarray([fourier_smoothing(obj,F_cutoff) for obj in vardict_valid[name]])\n",
    "        vardict_smooth_test[name] = np.asarray([fourier_smoothing(obj,F_cutoff) for obj in vardict_test[name]])\n",
    "\n",
    "    trainvalidexp_dict = {}\n",
    "    for name in unique_names:\n",
    "        # Concatenate the data\n",
    "        trainvalidexp_dict[name] = np.concatenate([vardict_smooth[name], vardict_smooth_valid[name]],axis=0)\n",
    "\n",
    "    pca_dict = {}\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "    for iname in unique_names:\n",
    "        pca, mean, std = preproc.train_PCA(trainvalidexp_dict[iname])\n",
    "        pca_dict[iname] = pca\n",
    "        mean_dict[iname] = mean\n",
    "        std_dict[iname] = std\n",
    "    os.makedirs(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/', exist_ok=True)\n",
    "    os.makedirs(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pca/', exist_ok=True)\n",
    "    os.makedirs(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/', exist_ok=True)\n",
    "    os.makedirs(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/scaler/', exist_ok=True)\n",
    "\n",
    "    os.makedirs(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/smooth{F_cutoff}', exist_ok=True)\n",
    "    baseline.save_models(pca_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pca/pcaall_smooth_F{F_cutoff}.pkl')\n",
    "    baseline.save_models(mean_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/scaler/meanall_smooth_F{F_cutoff}.pkl')\n",
    "    baseline.save_models(std_dict,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/scaler/stdall_smooth_F{F_cutoff}.pkl')\n",
    "\n",
    "    # Create a dictionary for each unique name\n",
    "    vardict_train = {\"name\": unique_names}\n",
    "    vardict_valid = {\"name\": unique_names}\n",
    "    \n",
    "    for i in range(7):\n",
    "        for name in unique_names:\n",
    "            vardict_train[name] = trainvalidexp_dict[name][train_sets[i]]\n",
    "            vardict_valid[name] = trainvalidexp_dict[name][val_sets[i]]\n",
    "        # Store the data\n",
    "        baseline.save_models(vardict_train,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/smooth{F_cutoff}/tsall_train_split_{i}.pkl')\n",
    "        baseline.save_models(vardict_valid,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/smooth{F_cutoff}/tsall_val_split_{i}.pkl')\n",
    "        \n",
    "    vardict_trains = []\n",
    "    vardict_valids = []    \n",
    "    for i in range(7):\n",
    "        vardict_trains.append(baseline.load_pickle(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/smooth{F_cutoff}/tsall_train_split_{i}.pkl'))\n",
    "        vardict_valids.append(baseline.load_pickle(f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/ts/smooth{F_cutoff}/tsall_val_split_{i}.pkl'))\n",
    "\n",
    "    PCloadings_train = []\n",
    "    PCloadings_valid = []\n",
    "    PCloadings_test = []\n",
    "\n",
    "    # Produce the PCA transformed data\n",
    "    for iseed in tqdm(range(7)):\n",
    "        PCdicts_train = {}\n",
    "        PCdicts_valid = {}\n",
    "        PCdicts_test = {}\n",
    "        for iname in unique_names:\n",
    "            #--------- FIHT: Remove this methods because it overcomplicates FFS ----------------\n",
    "            # # Find the number of components that explain 99% of the variance\n",
    "            atg = np.abs(pca_dict[iname].explained_variance_ratio_.cumsum()-varsexp).argmin()\n",
    "            #atg = 10\n",
    "            # Load the trained mean and standard deviation\n",
    "            trainmean = mean_dict[iname]\n",
    "            # Read the train/valid/test data\n",
    "            traindata = vardict_trains[iseed][iname]\n",
    "            validdata = vardict_valids[iseed][iname]\n",
    "            testdata = vardict_smooth_test[iname]\n",
    "            # PCA transform\n",
    "            temptrain = pca_dict[iname].transform(traindata)[:,:atg+1]\n",
    "            tempvalid = preproc.myPCA_projection_sen(pca_dict,iname,validdata,trainmean)[:,:atg+1]\n",
    "            temptest =  preproc.myPCA_projection_sen(pca_dict,iname,testdata,trainmean)[:,:atg+1]\n",
    "            # Standardize the data\n",
    "            PCdicts_train[iname] = (temptrain-np.mean(temptrain))/np.std(temptrain)\n",
    "            PCdicts_valid[iname] = (tempvalid-np.mean(temptrain))/np.std(temptrain)\n",
    "            PCdicts_test[iname] = (temptest-np.mean(temptrain))/np.std(temptrain)\n",
    "        PCloadings_train.append(PCdicts_train)\n",
    "        PCloadings_valid.append(PCdicts_valid)\n",
    "        PCloadings_test.append(PCdicts_test)\n",
    "\n",
    "    baseline.save_models(PCloadings_train,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_smooth{F_cutoff}_train.pkl')\n",
    "    baseline.save_models(PCloadings_valid,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_smooth{F_cutoff}_valid.pkl')\n",
    "    baseline.save_models(PCloadings_test,f'../../datas/proc/sfs/PCcomp_var{varsexp_filename}/pcs/pcsall_smooth{F_cutoff}_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and concatenate y data\n",
    "ytrain_cdf = pd.read_csv(seed_doc +'y_train_cdf.csv').values\n",
    "ytrain_max = pd.read_csv(seed_doc +'y_train_max.csv').values\n",
    "yvalid_cdf = pd.read_csv(seed_doc +'y_validation_cdf.csv').values\n",
    "yvalid_max = pd.read_csv(seed_doc +'y_validation_max.csv').values\n",
    "ytest_cdf = pd.read_csv(seed_doc +'y_test_cdf.csv').values\n",
    "ytest_max = pd.read_csv(seed_doc +'y_test_max.csv').values\n",
    "# Concatenate the y data\n",
    "y_cdf = np.concatenate([ytrain_cdf, yvalid_cdf],axis=0)\n",
    "y_max = np.concatenate([ytrain_max, yvalid_max],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained\n",
    "varsexp = 0.75\n",
    "# Store the indices for different storms\n",
    "storm_indices = np.arange(0,56)\n",
    "# Set the random seed\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(storm_indices)\n",
    "# Create 7 folds\n",
    "kf = KFold(n_splits=7, shuffle=False, random_state=None)  # Shuffle was already done\n",
    "# Store the indices for each fold\n",
    "train_sets, val_sets = [], []\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(storm_indices)):\n",
    "    train_set = storm_indices[train_idx]\n",
    "    val_set = storm_indices[val_idx]\n",
    "    train_sets.append(train_set)\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 17.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19.07it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.63it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.04it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 13.39it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for F_cutoff in [1,3,5,7,9,11,13]:\n",
    "    do_smoothing_and_pca_and_make_PCs(vardict, vardict_valid, vardict_test, unique_names, F_cutoff, varsexp, train_sets, val_sets)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
